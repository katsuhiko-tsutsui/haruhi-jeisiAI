import os
from typing import List, Dict, Any, Tuple
import openai
from supabase_client import supabase

# ===============================
# OpenAIè¨­å®š
# ===============================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
EMBED_MODEL = "text-embedding-3-small"  # 1536æ¬¡å…ƒãƒ»å®‰ä¾¡ãƒ»æ•™è‚²ç”¨é€”å‘ã‘
CHAT_MODEL  = "gpt-4o"                  # å¿œç­”å“è³ªé‡è¦–ï¼ˆJEISIæ¨™æº–ï¼‰


# ===============================
# åŸ‹ã‚è¾¼ã¿é–¢æ•°ç¾¤
# ===============================
def _embed(texts: List[str]) -> List[List[float]]:
    """OpenAI APIã§ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ"""
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return [d.embedding for d in resp.data]

def _embed_one(text: str) -> List[float]:
    return _embed([text])[0]


# ===============================
# RAGã‚¨ãƒ³ã‚¸ãƒ³æœ¬ä½“
# ===============================
class RagEngine:
    """
    JEISIæ•™è‚²AI RAGãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆHARUHIï¼SAKURAå…±é€šåˆ©ç”¨ï¼‰
    """
    def __init__(self, top_k: int = 3, min_score: float = 0.55):
        self.top_k = top_k
        self.min_score = float(min_score)

    # ---------- FAQåŸ‹ã‚è¾¼ã¿æ›´æ–° ----------
    def backfill_faq_embeddings(self, batch_size: int = 200) -> int:
        """embeddingãŒNULLã®FAQã«åŸ‹ã‚è¾¼ã¿ã‚’ä»˜ä¸"""
        total = 0
        offset = 0
        while True:
            res = supabase.table("haruhi_faqs") \
                .select("id, question, answer") \
                .is_("embedding", None) \
                .range(offset, offset + batch_size - 1).execute()
            rows = res.data or []
            if not rows:
                break
            texts = [f"Q: {r['question']}\nA: {r['answer']}" for r in rows]
            vecs = _embed(texts)
            for r, v in zip(rows, vecs):
                supabase.table("haruhi_faqs").update({"embedding": v}).eq("id", r["id"]).execute()
            total += len(rows)
            offset += batch_size
        return total

    # ---------- FAQæ¤œç´¢ ----------
    def search_faqs(self, query: str, k=3) -> List[Dict[str, Any]]:
        """Supabase RPCã«ã‚ˆã‚‹FAQé¡ä¼¼æ¤œç´¢"""
        embedding = client.embeddings.create(model=EMBED_MODEL, input=query)
        qvec = embedding.data[0].embedding

        rpc = supabase.rpc("match_faqs", {
            "query_embedding": qvec,
            "match_count": k
        }).execute()

        print("ğŸ” match_faqs result:", rpc.data)
        return rpc.data or []

    # ---------- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ ----------
    def build_prompt(self, user_query: str, faqs: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        top = faqs[: self.top_k]
        ctx_lines = []
        for i, f in enumerate(top, 1):
            ctx_lines.append(f"[FAQ#{i}] Q: {f['question']}\nA: {f['answer']}")
        ctx = "\n\n".join(ctx_lines) if top else "ï¼ˆè©²å½“FAQãªã—ï¼‰"

        system = (
            "ã‚ãªãŸã¯JEISIã®æ•™è‚²æ€è€ƒæ”¯æ´AIã€HARUHIã€ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼SAKURAã§ã™ã€‚"
            "ä»¥ä¸‹ã®RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            "å¤–éƒ¨çŸ¥è­˜ã‚„ä¸€èˆ¬çš„ãªæ¨æ¸¬ã§ç½®ãæ›ãˆãšã€æ•™è‚²çš„ãƒ»å“²å­¦çš„æ–‡è„ˆã‚’é‡è¦–ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚"
            "æ ¹æ‹ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ã€ç§ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ã¯ãã®æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚"
        )
        rag_instructions = (
            "=== RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ===\n"
            f"{ctx}\n"
            "======================="
        )
        user = f"{rag_instructions}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•:\n{user_query}"

        return [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ]

    # ---------- å¿œç­”ç”Ÿæˆ ----------
    def generate(self, messages: List[Dict[str, str]]) -> str:
        """GPTãƒ¢ãƒ‡ãƒ«ã§æœ€çµ‚å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            resp = client.chat.completions.create(
                model=CHAT_MODEL,
                messages=messages,
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            print("âŒ generate() error:", e)
            return "å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    # ---------- çµ±åˆå‡¦ç† ----------
    def answer_with_rag(self, user_query: str) -> Tuple[str, Dict[str, Any]]:
        """RAGçµ±åˆå‡¦ç†ï¼ˆæ¤œç´¢ â†’ ãƒ•ã‚£ãƒ«ã‚¿ â†’ å¿œç­”ç”Ÿæˆï¼‰"""
        faqs = self.search_faqs(user_query, k=self.top_k)
        print("ğŸ“Š faqs type:", type(faqs))
        print("ğŸ“Š faqs content:", faqs)
        if faqs:
            print("ğŸ§© keys of first item:", faqs[0].keys())
        print("ğŸ¯ min_score =", self.min_score)

        # å®‰å…¨ã‚­ãƒ£ã‚¹ãƒˆï¼ˆDecimalå¯¾ç­–ï¼‰
        def safe_score(f):
            try:
                return float(f.get("score", 0) or 0)
            except Exception:
                return 0.0

        faqs = [f for f in faqs if safe_score(f) >= self.min_score]
        print("âœ… Filtered FAQs:", faqs)

        # RAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        messages = self.build_prompt(user_query, faqs)
        # GPTå¿œç­”ç”Ÿæˆ
        reply = self.generate(messages)

        meta = {
            "used_faqs": [{"id": f["id"], "question": f["question"], "score": safe_score(f)} for f in faqs]
        }
        return reply, meta
